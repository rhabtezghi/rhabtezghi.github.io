{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Introduction",
   "metadata": {
    "cell_id": "4ef4b2e9d16a4a32a147701150e1f7c7",
    "tags": [],
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "The following code entails all the function and methods we used to create our final project. It is currently run on an example dataset of size 50. \\\nThe dataset used in this project can be found under https://www.agriculture-vision.com/agriculture-vision-2021/dataset-2021 and can be downloaded via the aws cli. ",
   "metadata": {
    "cell_id": "bcdf5a7076ff4cf08f918c83fe42fafb",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 119.59375
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Packages &amp; Directories",
   "metadata": {
    "cell_id": "ca6629d905304ead837df80f327ff36a",
    "tags": [],
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6a41daaa-cfbe-4485-8688-4344816b6b4b",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 156
   },
   "source": "!pip install pandas\n!pip install sklearn\n!pip install tensorflow==2.7\n!pip install keras==2.7\n!pip install scipy\n!pip install tensorflow-addons",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00001-dea4e125-8f95-4a5d-a20d-7bfc6214ca46",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 462
   },
   "source": "# Packages\nimport os\nimport time\nimport numpy as np\nimport pandas as pd \nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom scipy.stats import randint as sp_randint\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nimport keras\nfrom tensorflow import keras\nimport keras.layers as layers\nfrom keras.models import Sequential,Input,Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.advanced_activations import LeakyReLU\nimport tensorflow as tf\nimport tensorflow_addons as tfa",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00002-8ffe8349-1242-4f28-a1b5-166f7900ce22",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 282
   },
   "source": "# Different directory variables depending on stage of training/testing change to fit your environment\n\ndirectory_image_train = \"train/images/rgb\"\ndirectory_mask_train = \"train/masks\"\ndirectory_boundaries_train = \"train/boundaries\"\ndirectory_labels_train = \"train/labels/\"\n\ndirectory_image_validation = \"val/images/rgb\"\ndirectory_mask_validation = \"val/masks\"\ndirectory_boundaries_validation = \"val/boundaries\"\ndirectory_labels_validation = \"val/labels/\"\n\nsize = 50 #use to limit dataset size",
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Labeling the dataset",
   "metadata": {
    "cell_id": "00003-730a5c14-c528-4987-8e8e-53ff38de55c1",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-b629d916-34ad-4a47-a185-0576de32f494",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1326
   },
   "source": "def labeling_picture(directory, size): \n    '''\n    Takes in a directory and a size of the data set to be labelled.\n    Returns: labeled pictures\n    '''\n    \n    # (1) 'label_list' gets the files in the inputted directory and returns a list of files in that directory.\n        # So if we put the 'labels' folder as directory, it will list the 9 labels (or folders) that are in there.\n    # (2) '_label' is the numerical value that we will give to the category/label \n    # (3) 'categories_df' is an empty list ...\n    # (4) 'pictures' is an empty list where the pictures will be added that we loop through below.\n    \n    label_list = os.listdir(directory)\n    _label = 0 \n    categories_df = []\n    pictures = []\n    _break = 0\n\n    # Looping through the labels (folder level)\n    # For each label or folder in this list that we retrieved in the previous step:\n    # (1) print the label\n    # (2) '_picture' is the number of the picture in the sorted list\n    # (3) 'category' is a list long enough to hold all picture labels to be appended to a dataframe\n    # (4) 'pic_list' is a variable which returns a list of files in the folders of the label (so one level deeper) and store it in pic_list\n    \n    for label in sorted(label_list):\n\n        if _break == size: \n            break\n        \n        pic_list = os.listdir(directory+label)\n        print(label) \n        _picture = 0 \n        category = [None] * size # or len(pic_list) if run on all data\n        \n        #looping through the pictures in the label folder\n        for picture in sorted(pic_list):\n            \n            if _break == size: \n                break\n            pictures.append(picture)\n            #check if picture has two colors by checking if all pixesl are on average black(0)\n            img = Image.open(directory+label+\"/\"+picture)\n            if np.average(img) != 0: \n\n                #add label to category matrix   \n                category[_picture] = 1\n            else: \n                category[_picture] = 0 \n            _picture += 1\n            _break += 1\n\n        _break = 0\n        _label += 1 \n        categories_df.append(category)\n\n    # Create a dataframe with all labeled pictures \n    y = pd.DataFrame({\n                \"double_plant\": categories_df[0],\n                \"drydown\": categories_df[1],\n                \"endrow\": categories_df[2],\n                \"nutrient_deficiency\": categories_df[3],\n                \"planter_skip\": categories_df[4],\n                \"storm_damage\": categories_df[5],\n                \"water\": categories_df[6],\n                \"waterway\": categories_df[7], \n                \"weed_cluster\": categories_df[8], \n                 }, \n                 index = np.unique(pictures))\n\n    return y",
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00005-96add974-7480-4684-8fcd-4f2ef467c87c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 518.515625
   },
   "source": "#getting the labeled data for training\ny_train_full = labeling_picture(directory_labels_train, size)\n#labelling the validation & test data \ny_val_and_test = labeling_picture(directory_labels_validation, size)\n",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "double_plant\ndrydown\nendrow\nnutrient_deficiency\nplanter_skip\nstorm_damage\nwater\nwaterway\nweed_cluster\ndouble_plant\ndrydown\nendrow\nnutrient_deficiency\nplanter_skip\nstorm_damage\nwater\nwaterway\nweed_cluster\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Data preprocessing and cleaning",
   "metadata": {
    "cell_id": "00006-39198a7d-eb0a-41ad-8d9d-271cfeae89dc",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00007-0b5b24c2-3848-46e7-80c5-3156b9287743",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 210
   },
   "source": "# This function takes the arrays of the RGB image, the boundary and the mask as input\n# It returns the product of the three arrays, meaning these images will be the final input\n\ndef masking_bounding (np_image, np_bound, np_mask):\n    '''\n    Return the image RBG values after multiplication with boundary and mask values. \n    '''\n    masked_np = np.multiply(np_image, np_bound, np_mask)\n    return masked_np",
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00008-c49116ae-716c-4be6-ac3f-7ba19e354239",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 426
   },
   "source": "# This function is used to filter out the distorted/invalid pictures that should not be used for input\n# The function returns a list of three elements corresponding to avg RBG values respectivly. If these exceed the limits, then the image will be considered non-usable.\n\ndef filter_rbg(image_value, image_number):\n    \"\"\"\n    Takes in the np_image variable from calc_masked_boundaries and returns average RBG values. After \n    that it checks for anomialies and returns a list of the distorted pictures. \n    \"\"\"\n    avg_val = np.average(image_value, axis = (1, 2)) # returns a list of three elements corresponding to avg RBG values respectivly\n\n    upper = 250 # upper limit\n    lower = 5 # lower limit\n    filter_TF = False\n\n    for val in avg_val:\n        if val < lower or val > upper:\n            filter_TF = True\n            break\n\n\n    return filter_TF, image_number",
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00009-d99bc56c-97b5-4d58-a20d-93ed76f58e27",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1362
   },
   "source": "def calc_masked_boundaries(image_path, mask_path, boundaries_path, size):\n    \"\"\"\n    Taking the raw pictures as input and outputting them as input format for PCA/Alexnet.\n    The funciton also calls the filetering function (filter_rbg) to filter out bad pictures. \n    After that it scales the pixels values to a range between 0 and 1. \n\n    Returns a tuple of an array and a tensor that needs to be unpacked into two variables \n    before it can be used in either PCA or Alexnet. The array is located at position 0 and\n    is used for the random forrest and the tensor is located at position 1 and is used for\n    the AlexNet.\n\n    \"\"\"\n\n    #creating a list of the images to be processed \n    image_lst = os.listdir(image_path)\n\n    #creating a list of the images & necessary variables\n    X = []\n    image_num = 0 \n    distorted_pics = []\n    _break = 0\n\n    #looping through the pictures\n    for image in sorted(image_lst):\n\n        if _break == size: \n            break\n\n        image = image[:-4]\n\n        #access the mask \n        mask_img = Image.open(mask_path+\"/\"+ image + \".png\").convert(\"RGB\")\n\n        #access the boundaries \n        boundaries_img = Image.open(boundaries_path + \"/\" + image + \".png\").convert(\"RGB\")\n\n        #accessing the image \n        image_img = Image.open(image_path + \"/\" + image +\".jpg\").convert(\"RGB\")\n\n        #numpy conversion\n        np_image = np.asarray(image_img)\n        np_mask = np.asarray(mask_img)\n        np_boundaries = np.asarray(boundaries_img)\n\n        #Filter picture in order to make sure that RBG values are not distorted\n        filter_TF, img_index = filter_rbg(np_image, image_num)\n        if filter_TF == True:\n            distorted_pics.append(img_index)\n            image_num += 1\n            _break += 1\n        else: \n            #apply mask and boundary to original image\n            masked_bound_img = masking_bounding(np_image, np_boundaries, np_mask)\n\n            #normalizing the image         \n            norm_img = tf.image.resize(masked_bound_img, (277,277))\n\n            #standardizing the image to values between 0 - 1\n            norm_img = norm_img / 255\n\n            #appending normalised picture to the array\n            X.append(norm_img)\n            \n            #necessary variables to check if dataset size is reached and keep count of \n            _break += 1\n            print(image_num)\n            image_num += 1\n    \n    #creating the Random Forest and CNN input\n    X_rf = np.array(X) # Random Forest\n    X = tf.convert_to_tensor(X) # CNN\n    \n    return X_rf, X, distorted_pics",
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00010-249455ea-31c3-4945-a581-7b565456c21b",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1758
   },
   "source": "def calc_masked_boundaries_val_test(image_path, mask_path, boundaries_path, y_val_test, size):\n    \"\"\"\n    Taking the raw pictures as input and outputting them as validation & test input format for PCA/Alexnet.\n    The funciton also calls the filetering function (filter_rbg) to filter out bad pictures.\n    After that it scales the pixels values to a range between 0 and 1.\n\n    Returns a tuple of an array and a tensor that needs to be unpacked into two variables\n    before it can be used in either PCA or Alexnet. The array is located at position 0 and\n    is used for the random forrest and the tensor is located at position 1 and is used for\n    the AlexNet.\n\n    \"\"\"\n\n    # creating a list of the images to be processed\n    image_lst = os.listdir(image_path)\n\n    # creating a list of the images & necessary variables\n    X_val = [] #holding image details of validation data\n    X_test = [] #holding image details of test data\n    image_num = 0 #image number \n    _break = 0 #use for limited data set if no limit then comment out\n    distorted_pics = [] #list of distorted pics (indicies)\n    df_y_val = pd.DataFrame() #Dataframe holding all validation data labels\n    df_y_test = pd.DataFrame() #Dataframe holding all test data labels\n\n    # looping through the pictures\n    for image in sorted(image_lst):\n\n        if _break == size:\n         break\n\n        image = image[:-4]\n\n        # access the mask\n        mask_img = Image.open(mask_path + \"/\" + image + \".png\").convert(\"RGB\")\n\n        # access the boundaries\n        boundaries_img = Image.open(boundaries_path + \"/\" + image + \".png\").convert(\n            \"RGB\"\n        )\n\n        # accessing the image\n        image_img = Image.open(image_path + \"/\" + image + \".jpg\").convert(\"RGB\")\n\n        # numpy conversion\n        np_image = np.asarray(image_img)\n        np_mask = np.asarray(mask_img)\n        np_boundaries = np.asarray(boundaries_img)\n\n        # Filter picture in order to make sure that RBG values are not distorted\n        filter_TF, img_index = filter_rbg(np_image, image_num)\n        if filter_TF == True:\n            distorted_pics.append(img_index)\n            image_num += 1\n            _break += 1\n        else:\n\n            # calculating the masked and boundaries imgs for validation & test\n            masked_np = np.multiply(np_image, np_mask)\n            bound_masked_np = np.multiply(np_image, np_boundaries, np_mask)\n\n            # normalizing the images for the two sets\n            norm_img_test = tf.image.resize(masked_np, (277, 277))\n            norm_img_val = tf.image.resize(bound_masked_np, (277, 277))\n\n            # standardizing the image to values between 0 - 1\n            norm_img_test = norm_img_test / 255\n            norm_img_val = norm_img_val / 255\n\n            # appending normalised picture to the array of validation & test data\n            if image_num % 2 == 0:\n                X_val.append(norm_img_val)\n                entry = y_val_test.loc[image + \".png\"]\n                df_y_val = pd.concat([df_y_val, entry], axis=1)\n\n            else:\n                X_test.append(norm_img_test)\n                entry = y_val_test.loc[image + \".png\"]\n                df_y_test = pd.concat([df_y_test, entry], axis=1)\n\n            # necessary variables to check if dataset size is reached and keep count of\n            _break += 1\n            image_num += 1\n        # print(image_num)\n        mask_img.close()\n        boundaries_img.close()\n        image_img.close()\n\n    # creating the Random Forest and CNN input\n    X_rf_val = np.array(X_val)\n    X_rf_test = np.array(X_test)\n    X_val = tf.convert_to_tensor(X_val)\n    X_test = tf.convert_to_tensor(X_test)\n\n    return X_rf_val, X_rf_test, X_val, X_test, distorted_pics, df_y_val, df_y_test",
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00011-c2c17c1b-185d-482e-9dd7-9fd936accd59",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 773
   },
   "source": "# This cell tests the calc_masked_boundaries function.\n# It unpacks the tuple (immutable list) of the three arrays and assigns them to the three variables respectively\n\nX_rf_train, X_cnn_train, distort_pics = calc_masked_boundaries(directory_image_train,directory_mask_train, directory_boundaries_train, size)\nprint(\"Filtered pictures: \",len(distort_pics))",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n34\n35\n37\n38\n39\n40\n41\n42\n45\n46\n47\n48\n49\nFiltered pictures:  4\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00012-43bc9b5a-4bb3-4288-94e2-e4f675f484d0",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 138
   },
   "source": "# This cell drops the filtered datapoints of the training set.\n# It goes into the dataframe of labels for the training set and drops the distorted pictures.\n# We use \"inplace = True\" so the data is modified in place and nothing is returned. It merely updates the dataframe. \n\ny_train_full.drop(y_train_full.index[distort_pics], inplace = True )",
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00013-687d3c11-f6fc-4085-879e-5a75c011a97c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 102
   },
   "source": "# This cell tests the calc_maskes_boundaries_val_train function\n# It unpacks the tuple (immutable list) of the five arrays and assigns them to the five variables respectively\nX_rf_val, X_rf_test, X_val, X_test, distorted_pics, df_y_val, df_y_test= calc_masked_boundaries_val_test(directory_image_validation,directory_mask_validation, directory_boundaries_validation, y_val_and_test, size)",
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00014-af02a42a-b212-457d-bae5-8485703f580e",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 408.171875
   },
   "source": "print(X_cnn_train.shape)\nprint(y_train_full.shape)\nprint(X_val.shape)\ny_val = df_y_val.transpose()\ny_val = np.array(y_val).astype(\"int\") \ny_val = tf.convert_to_tensor(y_val)\nprint(y_val.shape)\ny_test = df_y_test.transpose()\ny_test = np.array(y_test).astype(\"int\") \ny_test = tf.convert_to_tensor(y_test)\nprint(y_test.shape) \nprint(X_test.shape)",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(46, 277, 277, 3)\n(46, 9)\n(25, 277, 277, 3)\n(25, 9)\n(25, 9)\n(25, 277, 277, 3)\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### AlexNet unmodified",
   "metadata": {
    "cell_id": "00015-3b1fc9ef-4a5a-4544-a329-b1b1b3fcf138",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00016-52b7e21c-9ce5-468f-8bda-31a1a104b382",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 984
   },
   "source": "# AlexNet model unmodified\n# In order to use AlexNet we use the TensorFlow library. \n# On top of the Tensorflow libarary AlexNet uses the Keras library/API to build its architecture. \n\n# The basic AlexNet model is a sequential model which is appropriate for stacking layers. We create this model here. \nmodel = keras.Sequential()\n\n# The basic AlexNet model has 5 convolutional layers (Conv2D) which use the relu activation function (using this function speeds up the training process).\n# Filters: the number of filters used in the convolutional layers changes throughout the network. As we go deeper the number of filters is increasing, meaning it is extracting more features as we go through the network. \n# Kernel size: the kernel size also varies throughout the architecture. The kernel size decreases the deeper we go into the network, decreasing the size of the shape of the feature map. \n# Strides: the stride controls how the filter convolves around the input volume and represents how much the filter shifts. In AlexNet this decreases for 4 to 1, becoming more precise. \n# Input shape: the input shape of our images. \n# Padding: Layers 2-5 include padding to prevent the size of the feature maps from reducing drastically\n\n# After each new layer, BatchNormalization is applied to avoid the gradient becoming too small. \n\n# After the first two and the last convolutional layer, a pooling layer is included to summarize the learned learned image features in the previous layer. \n# Pool size: this says something about how big the region is that the maximum will be taken from. The smaller the size, the more specific it is. \n# Strides: represents how big the area shifts with each step.\n\n# Flatten: The five convolutional layers are followed by a flatten layer which converts all the 2d arrays that resulted from the pooled feature maps into one long linear vector. This is needed to push the input through to the fully connected layers\n# Next up we have two fully connected layers which include 4096 neurons and have the relu function as the activation function. \n# Dropout: 0.5 means that in training a neuron will be connected to only half as many input neurons as it would when testing.\n# The final activation function is the softmax function, which has proved to work well with multiclass classification. \nmodel.add(layers.Conv2D(filters=96, kernel_size=(11, 11), \n                        strides=(4, 4), activation=\"relu\", \n                        input_shape=(277, 277, 3)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool2D(pool_size=(3, 3), strides= (2, 2)))\nmodel.add(layers.Conv2D(filters=256, kernel_size=(5, 5), \n                        strides=(1, 1), activation=\"relu\", \n                        padding=\"same\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\nmodel.add(layers.Conv2D(filters=384, kernel_size=(3, 3), \n                        strides=(1, 1), activation=\"relu\", \n                        padding=\"same\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Conv2D(filters=384, kernel_size=(3, 3), \n                        strides=(1, 1), activation=\"relu\", \n                        padding=\"same\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Conv2D(filters=256, kernel_size=(3, 3), \n                        strides=(1, 1), activation=\"relu\", \n                        padding=\"same\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(4096, activation=\"relu\"))\nmodel.add(layers.Dense(4096, activation=\"relu\"))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(9, activation=\"softmax\"))",
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00017-ceeb0cf9-0c07-4ecd-8ba2-b25fa08dff18",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 932.890625
   },
   "source": "# In this cell we configure the model with the loss functions, optimizers and the metrics.\n# The standard loss function of AlexNet is the BinaryCrossentropy\n# from_logits = False since we want a proabability (i.e. a value between [0,1])\n# We use the Stochastic Gradient Descent as optimizer.\n# The metric we use to evaluate our model is the MeanIoU\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n              optimizer=tf.optimizers.SGD(lr=0.001),\n              metrics=[tf.keras.metrics.MeanIoU(num_classes=9)])\nmodel.summary()",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_10 (Conv2D)          (None, 67, 67, 96)        34944     \n                                                                 \n batch_normalization_10 (Bat  (None, 67, 67, 96)       384       \n chNormalization)                                                \n                                                                 \n max_pooling2d_6 (MaxPooling  (None, 33, 33, 96)       0         \n 2D)                                                             \n                                                                 \n conv2d_11 (Conv2D)          (None, 33, 33, 256)       614656    \n                                                                 \n batch_normalization_11 (Bat  (None, 33, 33, 256)      1024      \n chNormalization)                                                \n                                                                 \n max_pooling2d_7 (MaxPooling  (None, 16, 16, 256)      0         \n 2D)                                                             \n                                                                 \n conv2d_12 (Conv2D)          (None, 16, 16, 384)       885120    \n                                                                 \n batch_normalization_12 (Bat  (None, 16, 16, 384)      1536      \n chNormalization)                                                \n                                                                 \n conv2d_13 (Conv2D)          (None, 16, 16, 384)       1327488   \n                                                                 \n batch_normalization_13 (Bat  (None, 16, 16, 384)      1536      \n chNormalization)                                                \n                                                                 \n conv2d_14 (Conv2D)          (None, 16, 16, 256)       884992    \n                                                                 \n batch_normalization_14 (Bat  (None, 16, 16, 256)      1024      \n chNormalization)                                                \n                                                                 \n max_pooling2d_8 (MaxPooling  (None, 7, 7, 256)        0         \n 2D)                                                             \n                                                                 \n flatten_2 (Flatten)         (None, 12544)             0         \n                                                                 \n dense_6 (Dense)             (None, 4096)              51384320  \n                                                                 \n dense_7 (Dense)             (None, 4096)              16781312  \n                                                                 \n dropout_2 (Dropout)         (None, 4096)              0         \n                                                                 \n dense_8 (Dense)             (None, 9)                 36873     \n                                                                 \n=================================================================\nTotal params: 71,955,209\nTrainable params: 71,952,457\nNon-trainable params: 2,752\n_________________________________________________________________\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00018-19e629c5-df65-4cb2-ac9c-b056a900427f",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 446.96875
   },
   "source": "# In this cell we train our model on the training set. \n# We track how much time it takes to train\n# We specify that the algorithm will work through the entire training set 50 times. \n# We specify the data on which to evaluate the loss at the end of each epoch\n# We instruct it to first shuffle our entire dataset and to then make batches of size 64 --> we ise 64 training examples for each step in SGD. \n\nstart = time.time()\nmodel.fit(X_cnn_train,y_train_full,\n          epochs=2,\n          validation_data=(X_val,y_val),\n          validation_freq=1, \n          shuffle = True, \n          batch_size = 64)\nend = time.time()\nprint(end-start)",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/2\n1/1 [==============================] - 4s 4s/step - loss: 1.2024 - mean_io_u_2: 0.4444 - val_loss: 0.6958 - val_mean_io_u_2: 0.4356\nEpoch 2/2\n1/1 [==============================] - 3s 3s/step - loss: 0.8585 - mean_io_u_2: 0.4444 - val_loss: 0.6948 - val_mean_io_u_2: 0.4356\n6.900047063827515\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00019-ac844477-fd23-40be-8c4c-1c018613143d",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 181.875,
    "deepnote_output_heights": [
     null,
     21.1875
    ]
   },
   "source": "# In this cell we evaluate the performance of our model based on the predefined metrics\nmodel.evaluate(X_test,y_test)",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1/1 [==============================] - 0s 369ms/step - loss: 0.6951 - mean_io_u_2: 0.4422\n"
    },
    {
     "data": {
      "text/plain": "[0.6950905323028564, 0.4422222077846527]"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### AlexNet modified",
   "metadata": {
    "cell_id": "00020-454531c3-95f2-4ba2-8d95-9fc2644bfe5a",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00021-f0ad2adb-6e1b-4033-8687-2f6e426c2280",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 660
   },
   "source": "# AlexNet model modified\n\n# In this cell we re-define the AlexNet architecture. \n\nmodel_AN = keras.Sequential()\n\nmodel_AN.add(layers.Conv2D(filters=96, kernel_size=(11, 11), \n                        strides=(4, 4), activation=\"relu\", \n                        input_shape=(277, 277, 3)))\nmodel_AN.add(layers.BatchNormalization())\nmodel_AN.add(layers.MaxPool2D(pool_size=(3, 3), strides= (2, 2)))\nmodel_AN.add(layers.Conv2D(filters=256, kernel_size=(5, 5), \n                        strides=(1, 1), activation=\"relu\", \n                        padding=\"same\"))\nmodel_AN.add(layers.BatchNormalization())\nmodel_AN.add(layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\nmodel_AN.add(layers.Conv2D(filters=384, kernel_size=(3, 3), \n                        strides=(1, 1), activation=\"relu\", \n                        padding=\"same\"))\nmodel_AN.add(layers.BatchNormalization())\nmodel_AN.add(layers.Conv2D(filters=384, kernel_size=(3, 3), \n                        strides=(1, 1), activation=\"relu\", \n                        padding=\"same\"))\nmodel_AN.add(layers.BatchNormalization())\nmodel_AN.add(layers.Conv2D(filters=256, kernel_size=(3, 3), \n                        strides=(1, 1), activation=\"relu\", \n                        padding=\"same\"))\nmodel_AN.add(layers.BatchNormalization())\nmodel_AN.add(layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\nmodel_AN.add(layers.Flatten())\nmodel_AN.add(layers.Dense(4096, activation=\"relu\"))\nmodel_AN.add(layers.Dense(4096, activation=\"relu\"))\nmodel_AN.add(layers.Dropout(0.5))\nmodel_AN.add(layers.Dense(9, activation=\"softmax\"))",
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00022-a1f4fdff-2f76-4771-8688-38bf386510b0",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1022.890625
   },
   "source": "# In this cell we configure the model with the loss functions, optimizers and the metrics.\n# Modification: we use the SigmoidFocalCrossEntropy loss function since it has shown to be extremely useful to deal with class imbalance\n# Alpha: this is the balancing factor. We have 9 different categories and the numbers represent their respective percentage of the whole. \n# Gamma: this is the modulating factor. It is standard to take a value of 2.  \n# SGD: We still use SGD, but now we introduce a learning rate in combination with nesterov accelerated learning and a decay rate. This is chosen to both optimize and generalize the process.  \n\n#including a learning rate schedule to have a decay in learning rate\nlearning_rate = 0.1\nepochs = 50\ndecay_rate = learning_rate / epochs\n\nmodel_AN.compile(loss=tfa.losses.SigmoidFocalCrossEntropy(alpha=[.1, .30, .04, .22, .04, .01,.04,.06,.19], gamma=2),\n              optimizer=tf.optimizers.SGD(lr=learning_rate, nesterov = True, decay = decay_rate),\n              metrics=[tf.keras.metrics.MeanIoU(num_classes=9)])\nmodel_AN.summary()",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_15 (Conv2D)          (None, 67, 67, 96)        34944     \n                                                                 \n batch_normalization_15 (Bat  (None, 67, 67, 96)       384       \n chNormalization)                                                \n                                                                 \n max_pooling2d_9 (MaxPooling  (None, 33, 33, 96)       0         \n 2D)                                                             \n                                                                 \n conv2d_16 (Conv2D)          (None, 33, 33, 256)       614656    \n                                                                 \n batch_normalization_16 (Bat  (None, 33, 33, 256)      1024      \n chNormalization)                                                \n                                                                 \n max_pooling2d_10 (MaxPoolin  (None, 16, 16, 256)      0         \n g2D)                                                            \n                                                                 \n conv2d_17 (Conv2D)          (None, 16, 16, 384)       885120    \n                                                                 \n batch_normalization_17 (Bat  (None, 16, 16, 384)      1536      \n chNormalization)                                                \n                                                                 \n conv2d_18 (Conv2D)          (None, 16, 16, 384)       1327488   \n                                                                 \n batch_normalization_18 (Bat  (None, 16, 16, 384)      1536      \n chNormalization)                                                \n                                                                 \n conv2d_19 (Conv2D)          (None, 16, 16, 256)       884992    \n                                                                 \n batch_normalization_19 (Bat  (None, 16, 16, 256)      1024      \n chNormalization)                                                \n                                                                 \n max_pooling2d_11 (MaxPoolin  (None, 7, 7, 256)        0         \n g2D)                                                            \n                                                                 \n flatten_3 (Flatten)         (None, 12544)             0         \n                                                                 \n dense_9 (Dense)             (None, 4096)              51384320  \n                                                                 \n dense_10 (Dense)            (None, 4096)              16781312  \n                                                                 \n dropout_3 (Dropout)         (None, 4096)              0         \n                                                                 \n dense_11 (Dense)            (None, 9)                 36873     \n                                                                 \n=================================================================\nTotal params: 71,955,209\nTrainable params: 71,952,457\nNon-trainable params: 2,752\n_________________________________________________________________\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00023-45847d71-cb60-49d1-a74d-515ecc683dc0",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 428.96875
   },
   "source": "# In this cell we train our model on the training set. \n\nstart = time.time()\n\nmodel_AN.fit(X_cnn_train,y_train_full,\n                     epochs=2,\n                     validation_data=(X_val,y_val),\n                     validation_freq=1, \n                     shuffle = True,\n                     batch_size = 64\n                     )\n\nend = time.time() \nprint(end-start)",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/2\n1/1 [==============================] - 3s 3s/step - loss: 4.1749 - mean_io_u_3: 0.7455 - val_loss: 16.8772 - val_mean_io_u_3: 0.3800\nEpoch 2/2\n1/1 [==============================] - 3s 3s/step - loss: 4.1749 - mean_io_u_3: 0.7455 - val_loss: 16.8772 - val_mean_io_u_3: 0.3800\n5.865911960601807\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00024-60315811-db38-44ea-a3dd-74860cb5c6ab",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 181.875,
    "deepnote_output_heights": [
     null,
     21.1875
    ]
   },
   "source": "# In this cell we evaluate the performance of our model based on the predefined metrics\nmodel_AN.evaluate(X_test,y_test)",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1/1 [==============================] - 0s 387ms/step - loss: 16.6242 - mean_io_u_3: 0.3867\n"
    },
    {
     "data": {
      "text/plain": "[16.624202728271484, 0.3866666555404663]"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Random Forest \n",
   "metadata": {
    "cell_id": "00025-9a1340aa-1e85-43d4-882a-c53c94a12423",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00026-1b660f53-ddb1-4f10-8f7c-178f7a0358ae",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 408
   },
   "source": "# We will be using the random forest classifier from the sklearn package. \n# For this the input must be reshaped from a 4D shape to a 2D shape. We must therefore reshape our input and we do that below. \n# We reshape from a by (1) extracting the arrays from the shape tuple and \n# (2) multiply the dimensions of the images with each other to form form one dimension.\n\n# nsamples = amount of images\n# nx = number of x pixels (277)\n# ny = number of y pixels (277)\n# nrgb = number of colour streams (3)\n\n# Changing the shape of the train set \nnsamples, nx, ny, nrgb = X_rf_train.shape\nX_rf_train2 = X_rf_train.reshape((nsamples,nx*ny*nrgb))\n# Changing the shape of the validation set\nnsamples, nx, ny, nrgb = X_rf_val.shape\nX_rf_val2 = X_rf_val.reshape((nsamples,nx*ny*nrgb))\n# Changing the shape of the test set\nnsamples, nx, ny, nrgb = X_rf_test.shape\nX_rf_test2 = X_rf_test.reshape((nsamples,nx*ny*nrgb))\n",
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00027-3ce73be7-8fa9-4fd8-ab10-ac402a09801f",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 701.1875,
    "deepnote_output_heights": [
     21.1875,
     264
    ]
   },
   "source": "# In this cell we perform PCA to reduce dimensionality and increase the speed of the runs\n\npca = PCA(n_components=19) #if size is used then n_components must be adjusted\nX_train_pca = pca.fit_transform(X_rf_train2)\nX_val_pca = pca.fit_transform(X_rf_val2)\nX_test_pca = pca.fit_transform(X_rf_test2)\n\n# Make the scree plot to see how well PCA captures the variance of the data. It is a plot of how much variance is explained.\n# Look for where the drop is, or where it does not improve much more. \nplt.grid()\nplt.plot(np.cumsum(pca.explained_variance_ratio_ * 100))\nplt.xlabel('Number of components')\nplt.ylabel('Explained variance')\nplt.savefig('Scree plot.png')\n\n# See how much variance is explained with the amount of components\nnp.cumsum(pca.explained_variance_ratio_ * 100)[-1]",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "96.72356"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light",
      "image/png": {
       "width": 382,
       "height": 262
      }
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00028-4a94daf2-4663-43fd-b3aa-3b844a1e38bd",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 298.46875
   },
   "source": "# We also need to change the shape of the y_val. It is not possible to transform a dataframe in a tensor, so this is a workaround\n\nprint(y_train_full.shape)\nprint(X_rf_test.shape)\nprint(y_test.shape)\nprint(X_test_pca.shape)\nprint(y_val.shape)",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(46, 9)\n(25, 277, 277, 3)\n(25, 9)\n(25, 19)\n(25, 9)\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00029-e8ca6b1d-4dd3-42c1-a86b-32f6ce8de193",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 102
   },
   "source": "# Initialise a random forest classifyer\nrfc = RandomForestClassifier(bootstrap=True,\n                              random_state=1)",
   "execution_count": 81,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00030-f5275a0b-d1b9-4dcd-af1f-24ce35ff4b46",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 318
   },
   "source": "# Specify parameters and distributions to sample from\n\n# max_depth: The maximum depth of the tree. It will take a random integer between 1 and 300.\n# max_features: The number of features to consider when looking for the best split. It will take a random integer between 1 and 11.\n# min_samples_split: The minimum number of samples required to split an internal node. It will take a random integer between 1 and 11.\n# min_samples_leaf: The minimum number of samples required to be at a leaf node. It will take a random integer between 1 and 11.\n# bootstrap: Whether bootstrap samples are used when building trees. Will it randomly choose now?\n# criterion: The function to measure the impurity of the split. “gini” for the Gini impurity and “entropy” for the Shannon information gain\n\nparam_dist = {\"max_depth\": sp_randint(1, 300), \n              \"max_features\": sp_randint(1, 11),\n              \"min_samples_split\": sp_randint(2, 11),\n              \"min_samples_leaf\": sp_randint(1, 11),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}",
   "execution_count": 82,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00031-8eb5aca4-05ab-490c-97a9-f52524ec1a47",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 192
   },
   "source": "# In this cell we perform a random search for the best values of the hyperparameters.\n\n# The input of the randomized search is the Random Forest Classifier.\n# param_distributions: takes a dictionary of parameter names that it needs to try. We want it to do it for the parameters we specified in the cell above.\n# n_iter: Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution. We chose it randomly ?? What happens if we increase/decrease it?\n\nrandom_search_clf = RandomizedSearchCV(rfc, param_distributions=param_dist,\n                                   n_iter=20)",
   "execution_count": 83,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00032-6c6f8634-fbcc-4c69-9cc9-72feecc0ccae",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 255.6875
   },
   "source": "start = time.time()\n\n# Fit the model with the best hyperparameters to our training data\nrandom_search_clf.fit(X_train_pca, y_train_full)\n\nend = time.time() \n\n# Print how much time it takes\nprint(end-start)",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "12.266495943069458\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00033-a0e6b6a8-7d0e-4580-a611-2b0ce7964fc4",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 232.171875,
    "deepnote_output_heights": [
     117.171875
    ]
   },
   "source": "#returning the best parameters\nrandom_search_clf.best_params_",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bootstrap': False,\n 'criterion': 'gini',\n 'max_depth': 142,\n 'max_features': 9,\n 'min_samples_leaf': 2,\n 'min_samples_split': 6}"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00034-502c31ef-98a6-486f-af24-e3ed759e5660",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 167.390625
   },
   "source": "# Checking if the shapes are similar\nprint(X_test_pca.shape)\nprint(y_test.shape)",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(25, 19)\n(25, 9)\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00035-2bbb2bef-17e9-489e-b679-9bb4e2ddfce4",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 325.875,
    "deepnote_output_heights": [
     null,
     21.1875
    ]
   },
   "source": "# Print accuracy of the random forest classifier\n# We will not look at this metric\npreds = random_search_clf.predict(X_test_pca)\nprint(\"Accuracy of the Random Forest: \", accuracy_score(y_test, preds))\n\n# Print the mean IoU metric\n# We will use this one\nmetric = tf.keras.metrics.MeanIoU(num_classes=9)\nmetric.update_state(y_test, preds)\nmetric.result().numpy",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy of the Random Forest:  0.0\n"
    },
    {
     "data": {
      "text/plain": "<bound method _EagerTensorBase.numpy of <tf.Tensor: shape=(), dtype=float32, numpy=0.39333335>>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00036-75bce26c-45d6-4bfe-a75d-5aa79886fb79",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 66
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=53d0d428-af3f-4841-98f7-12269979931d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "deepnote_notebook_id": "b87113e4-72c8-4734-add2-ac7ae2eed040",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}