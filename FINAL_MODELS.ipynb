{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00001-81b83777-e77c-4bc0-ac14-fd72419a44e0",
    "deepnote_cell_height": 758.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9963,
    "execution_start": 1653553229443,
    "source_hash": "549f8513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all packages\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "print(\"Successfully imported all packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-6212e93b-5c50-4515-8baf-e4a011deda14",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "83e962e4fe9d4c96862dcbae4eb5ce99",
    "deepnote_cell_height": 134.6875,
    "deepnote_cell_type": "code",
    "deepnote_table_invalid": false,
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 1,
     "pageSize": 10,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 569,
    "execution_start": 1653553242385,
    "source_hash": "359a5a06",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#importing the data and drop unnecessary columns\n",
    "data = pd.read_csv('socialmedia-disaster-tweets-DFE 3.csv', error_bad_lines=False)\n",
    "data = data.drop(columns=[\"_unit_id\", \"_golden\", \"_unit_state\", \"_trusted_judgments\",\"choose_one:confidence\", \"choose_one_gold\", \"_last_judgment_at\"])\n",
    "data[\"target\"] = data.apply(lambda row: 1 if row[\"choose_one\"]== \"Relevant\" else 0 ,axis =1)\n",
    "full_dataset = data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-b27b8f83-363d-4c6c-8c8e-f5054626de8f",
    "deepnote_cell_height": 69.6875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-bac7f5ad-a311-45b7-ac8d-a5184f613164",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00006-fa4dd0a4-14ca-4799-9135-5b382b680bb4",
    "deepnote_cell_height": 116.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1227,
    "execution_start": 1653555633949,
    "source_hash": "25fc42f9"
   },
   "outputs": [],
   "source": [
    "# Tokenization \n",
    "data[\"tweet_tokens\"] = data.apply(lambda row: row[\"text\"].split(), axis = 1)\n",
    "data[\"tweet_tokens_joined\"] = data.apply(lambda row: \" \".join(row[\"tweet_tokens\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-9545b97d-f686-4915-9c1b-c73f34c28089",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00008-167d7ffb-62e1-41bb-9943-726f1dba1e8f",
    "deepnote_cell_height": 689.3125,
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 0,
     "pageSize": 50,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1464,
    "execution_start": 1653555636384,
    "source_hash": "4dfb7349"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choose_one</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_tokens_joined</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>cleaned_tweets_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Just, happened, a, terrible, car, crash]</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>[Just, happened, terrible, car, crash]</td>\n",
       "      <td>Just happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[Our, Deeds, Reason, #earthquake, May, ALLAH, ...</td>\n",
       "      <td>Our Deeds Reason #earthquake May ALLAH Forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Heard, about, #earthquake, is, different, cit...</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>[Heard, #earthquake, different, cities,, stay,...</td>\n",
       "      <td>Heard #earthquake different cities, stay safe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, is, a, forest, fire, at, spot, pond,, ...</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>[forest, fire, spot, pond,, geese, fleeing, ac...</td>\n",
       "      <td>forest fire spot pond, geese fleeing across st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask., Canada]</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask., Canada]</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[All, residents, asked, 'shelter, place', noti...</td>\n",
       "      <td>All residents asked 'shelter place' notified o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>[Just, got, sent, photo, Ruby, #Alaska, smoke,...</td>\n",
       "      <td>Just got sent photo Ruby #Alaska smoke #wildfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[#RockyFire, Update, =&gt;, California, Hwy., 20,...</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>[#RockyFire, Update, =&gt;, California, Hwy., 20,...</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Apocalypse, lighting., #Spokane, #wildfires]</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>[Apocalypse, lighting., #Spokane, #wildfires]</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  choose_one keyword location  \\\n",
       "0   Relevant     NaN      NaN   \n",
       "1   Relevant     NaN      NaN   \n",
       "2   Relevant     NaN      NaN   \n",
       "3   Relevant     NaN      NaN   \n",
       "4   Relevant     NaN      NaN   \n",
       "5   Relevant     NaN      NaN   \n",
       "6   Relevant     NaN      NaN   \n",
       "7   Relevant     NaN      NaN   \n",
       "8   Relevant     NaN      NaN   \n",
       "9   Relevant     NaN      NaN   \n",
       "\n",
       "                                                text  tweetid  userid  target  \\\n",
       "0                 Just happened a terrible car crash      1.0     NaN       1   \n",
       "1  Our Deeds are the Reason of this #earthquake M...     13.0     NaN       1   \n",
       "2  Heard about #earthquake is different cities, s...     14.0     NaN       1   \n",
       "3  there is a forest fire at spot pond, geese are...     15.0     NaN       1   \n",
       "4             Forest fire near La Ronge Sask. Canada     16.0     NaN       1   \n",
       "5  All residents asked to 'shelter in place' are ...     17.0     NaN       1   \n",
       "6  13,000 people receive #wildfires evacuation or...     18.0     NaN       1   \n",
       "7  Just got sent this photo from Ruby #Alaska as ...     19.0     NaN       1   \n",
       "8  #RockyFire Update => California Hwy. 20 closed...     20.0     NaN       1   \n",
       "9           Apocalypse lighting. #Spokane #wildfires     21.0     NaN       1   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0          [Just, happened, a, terrible, car, crash]   \n",
       "1  [Our, Deeds, are, the, Reason, of, this, #eart...   \n",
       "2  [Heard, about, #earthquake, is, different, cit...   \n",
       "3  [there, is, a, forest, fire, at, spot, pond,, ...   \n",
       "4     [Forest, fire, near, La, Ronge, Sask., Canada]   \n",
       "5  [All, residents, asked, to, 'shelter, in, plac...   \n",
       "6  [13,000, people, receive, #wildfires, evacuati...   \n",
       "7  [Just, got, sent, this, photo, from, Ruby, #Al...   \n",
       "8  [#RockyFire, Update, =>, California, Hwy., 20,...   \n",
       "9      [Apocalypse, lighting., #Spokane, #wildfires]   \n",
       "\n",
       "                                 tweet_tokens_joined  \\\n",
       "0                 Just happened a terrible car crash   \n",
       "1  Our Deeds are the Reason of this #earthquake M...   \n",
       "2  Heard about #earthquake is different cities, s...   \n",
       "3  there is a forest fire at spot pond, geese are...   \n",
       "4             Forest fire near La Ronge Sask. Canada   \n",
       "5  All residents asked to 'shelter in place' are ...   \n",
       "6  13,000 people receive #wildfires evacuation or...   \n",
       "7  Just got sent this photo from Ruby #Alaska as ...   \n",
       "8  #RockyFire Update => California Hwy. 20 closed...   \n",
       "9           Apocalypse lighting. #Spokane #wildfires   \n",
       "\n",
       "                                      cleaned_tweets  \\\n",
       "0             [Just, happened, terrible, car, crash]   \n",
       "1  [Our, Deeds, Reason, #earthquake, May, ALLAH, ...   \n",
       "2  [Heard, #earthquake, different, cities,, stay,...   \n",
       "3  [forest, fire, spot, pond,, geese, fleeing, ac...   \n",
       "4     [Forest, fire, near, La, Ronge, Sask., Canada]   \n",
       "5  [All, residents, asked, 'shelter, place', noti...   \n",
       "6  [13,000, people, receive, #wildfires, evacuati...   \n",
       "7  [Just, got, sent, photo, Ruby, #Alaska, smoke,...   \n",
       "8  [#RockyFire, Update, =>, California, Hwy., 20,...   \n",
       "9      [Apocalypse, lighting., #Spokane, #wildfires]   \n",
       "\n",
       "                               cleaned_tweets_joined  \n",
       "0                   Just happened terrible car crash  \n",
       "1  Our Deeds Reason #earthquake May ALLAH Forgive us  \n",
       "2  Heard #earthquake different cities, stay safe ...  \n",
       "3  forest fire spot pond, geese fleeing across st...  \n",
       "4             Forest fire near La Ronge Sask. Canada  \n",
       "5  All residents asked 'shelter place' notified o...  \n",
       "6  13,000 people receive #wildfires evacuation or...  \n",
       "7  Just got sent photo Ruby #Alaska smoke #wildfi...  \n",
       "8  #RockyFire Update => California Hwy. 20 closed...  \n",
       "9           Apocalypse lighting. #Spokane #wildfires  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing the stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "data['cleaned_tweets'] = data.apply(lambda row: [w for w in row[\"tweet_tokens\"] if w not in stopset], axis = 1)\n",
    "data[\"cleaned_tweets_joined\"] = data.apply(lambda row: \" \".join(row[\"cleaned_tweets\"]), axis = 1)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-eeda3e79-3c00-4bf4-9439-abe05d5fd70f",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00010-15813b93-5231-4419-b8d1-8888d84a21e1",
    "deepnote_cell_height": 484.359375,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2263,
    "execution_start": 1653555638788,
    "source_hash": "3f5f4661"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choose_one</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_tokens_joined</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>cleaned_tweets_joined</th>\n",
       "      <th>lemmatized_tweets</th>\n",
       "      <th>lemmatized_tweets_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Just, happened, a, terrible, car, crash]</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>[Just, happened, terrible, car, crash]</td>\n",
       "      <td>Just happened terrible car crash</td>\n",
       "      <td>[Just, happened, terrible, car, crash]</td>\n",
       "      <td>Just happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[Our, Deeds, Reason, #earthquake, May, ALLAH, ...</td>\n",
       "      <td>Our Deeds Reason #earthquake May ALLAH Forgive us</td>\n",
       "      <td>[Our, Deeds, Reason, #earthquake, May, ALLAH, ...</td>\n",
       "      <td>Our Deeds Reason #earthquake May ALLAH Forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Heard, about, #earthquake, is, different, cit...</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>[Heard, #earthquake, different, cities,, stay,...</td>\n",
       "      <td>Heard #earthquake different cities, stay safe ...</td>\n",
       "      <td>[Heard, #earthquake, different, cities,, stay,...</td>\n",
       "      <td>Heard #earthquake different cities, stay safe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, is, a, forest, fire, at, spot, pond,, ...</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>[forest, fire, spot, pond,, geese, fleeing, ac...</td>\n",
       "      <td>forest fire spot pond, geese fleeing across st...</td>\n",
       "      <td>[forest, fire, spot, pond,, goose, fleeing, ac...</td>\n",
       "      <td>forest fire spot pond, goose fleeing across st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask., Canada]</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask., Canada]</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask., Canada]</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  choose_one keyword location  \\\n",
       "0   Relevant     NaN      NaN   \n",
       "1   Relevant     NaN      NaN   \n",
       "2   Relevant     NaN      NaN   \n",
       "3   Relevant     NaN      NaN   \n",
       "4   Relevant     NaN      NaN   \n",
       "\n",
       "                                                text  tweetid  userid  target  \\\n",
       "0                 Just happened a terrible car crash      1.0     NaN       1   \n",
       "1  Our Deeds are the Reason of this #earthquake M...     13.0     NaN       1   \n",
       "2  Heard about #earthquake is different cities, s...     14.0     NaN       1   \n",
       "3  there is a forest fire at spot pond, geese are...     15.0     NaN       1   \n",
       "4             Forest fire near La Ronge Sask. Canada     16.0     NaN       1   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0          [Just, happened, a, terrible, car, crash]   \n",
       "1  [Our, Deeds, are, the, Reason, of, this, #eart...   \n",
       "2  [Heard, about, #earthquake, is, different, cit...   \n",
       "3  [there, is, a, forest, fire, at, spot, pond,, ...   \n",
       "4     [Forest, fire, near, La, Ronge, Sask., Canada]   \n",
       "\n",
       "                                 tweet_tokens_joined  \\\n",
       "0                 Just happened a terrible car crash   \n",
       "1  Our Deeds are the Reason of this #earthquake M...   \n",
       "2  Heard about #earthquake is different cities, s...   \n",
       "3  there is a forest fire at spot pond, geese are...   \n",
       "4             Forest fire near La Ronge Sask. Canada   \n",
       "\n",
       "                                      cleaned_tweets  \\\n",
       "0             [Just, happened, terrible, car, crash]   \n",
       "1  [Our, Deeds, Reason, #earthquake, May, ALLAH, ...   \n",
       "2  [Heard, #earthquake, different, cities,, stay,...   \n",
       "3  [forest, fire, spot, pond,, geese, fleeing, ac...   \n",
       "4     [Forest, fire, near, La, Ronge, Sask., Canada]   \n",
       "\n",
       "                               cleaned_tweets_joined  \\\n",
       "0                   Just happened terrible car crash   \n",
       "1  Our Deeds Reason #earthquake May ALLAH Forgive us   \n",
       "2  Heard #earthquake different cities, stay safe ...   \n",
       "3  forest fire spot pond, geese fleeing across st...   \n",
       "4             Forest fire near La Ronge Sask. Canada   \n",
       "\n",
       "                                   lemmatized_tweets  \\\n",
       "0             [Just, happened, terrible, car, crash]   \n",
       "1  [Our, Deeds, Reason, #earthquake, May, ALLAH, ...   \n",
       "2  [Heard, #earthquake, different, cities,, stay,...   \n",
       "3  [forest, fire, spot, pond,, goose, fleeing, ac...   \n",
       "4     [Forest, fire, near, La, Ronge, Sask., Canada]   \n",
       "\n",
       "                            lemmatized_tweets_joined  \n",
       "0                   Just happened terrible car crash  \n",
       "1   Our Deeds Reason #earthquake May ALLAH Forgive u  \n",
       "2  Heard #earthquake different cities, stay safe ...  \n",
       "3  forest fire spot pond, goose fleeing across st...  \n",
       "4             Forest fire near La Ronge Sask. Canada  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using lemmatization\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "data['lemmatized_tweets'] = data.apply(lambda row: [wnl.lemmatize(w) for w in row[\"cleaned_tweets\"]], axis = 1)\n",
    "data[\"lemmatized_tweets_joined\"] = data.apply(lambda row: \" \".join(row[\"lemmatized_tweets\"]), axis = 1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-9b66f9a2-54ab-43e0-a12c-65ce0bb40375",
    "deepnote_cell_height": 69.6875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-80e70666-a25b-426d-8f76-668d5812ce62",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "bdeba374060b4f50bfb42ad99502cc9b",
    "deepnote_cell_height": 80.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 3,
    "execution_start": 1653508718591,
    "source_hash": "746a4dbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "a2bb91df4a74414f965231d1767ae688",
    "deepnote_cell_height": 346.09375,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     231.546875
    ],
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 2,
    "execution_start": 1653508839537,
    "source_hash": "db465758",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train validation test split. The final models have been trained with only train & test set. Validation set has been only used for hyperparameter optimization prior final training\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"lemmatized_tweets_joined\"], data[\"target\"], test_size = 0.3, shuffle= True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "cell_id": "00016-75145f40-72e3-49dc-bc10-e7fe437eafa3",
    "deepnote_cell_height": 170.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 9,
    "execution_start": 1653508840224,
    "source_hash": "4c586e07"
   },
   "outputs": [],
   "source": [
    "#multinomial naive bayes pipeline\n",
    "NB_pipe_clf = Pipeline(\n",
    "    [('vect', CountVectorizer(decode_error='ignore', stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('nb_clf', MultinomialNB())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "cell_id": "00017-06174867-d1f3-449f-8832-22d50176fc8b",
    "deepnote_cell_height": 384.0625,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 494,
    "execution_start": 1653508841982,
    "source_hash": "cb7c8045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8061787421846267\n",
      "Precision:  0.8425821064552661\n",
      "Recall:  0.657243816254417\n",
      "F1-Measure:  0.7384615384615385\n"
     ]
    }
   ],
   "source": [
    "# training the model \n",
    "nb_clf = NB_pipe_clf.fit(X_train, y_train)\n",
    "\n",
    "#predicting \n",
    "preds = NB_pipe_clf.predict(X_test)\n",
    "\n",
    "\n",
    "#evaluation\n",
    "print(\"Accuracy: \", accuracy_score(y_test, preds))\n",
    "print(\"Precision: \", precision_score(y_test, preds))\n",
    "print(\"Recall: \", recall_score(y_test, preds))\n",
    "print(\"F1-Measure: \", f1_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4e494d5806e840dcb0fd988cb491a1db",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "cell_id": "79df39a08b024f2f863e40eff860ade0",
    "deepnote_cell_height": 170.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 6,
    "execution_start": 1653508847955,
    "source_hash": "60af66b5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SVM pipeline\n",
    "SVM_pipe_clf = Pipeline(\n",
    "    [('vect', CountVectorizer(decode_error='ignore', stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm_clf', LinearSVC())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "cell_id": "456477eb48534466bc4b003134d97473",
    "deepnote_cell_height": 348.0625,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 312,
    "execution_start": 1653508900580,
    "source_hash": "662f76c7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7910996689959544\n",
      "Precision:  0.7625698324022346\n",
      "Recall:  0.7234982332155477\n",
      "F1-Measure:  0.7425203989120581\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "svm_clf = SVM_pipe_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting\n",
    "svm_preds = SVM_pipe_clf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy: \", accuracy_score(y_test, svm_preds))\n",
    "print(\"Precision: \", precision_score(y_test, svm_preds))\n",
    "print(\"Recall: \", recall_score(y_test, svm_preds))\n",
    "print(\"F1-Measure: \", f1_score(y_test, svm_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-dae37f1a-0da5-4174-8914-bec0d65ccd5b",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "00019-2f7a1ef4-af3e-4c74-ad52-dabdd756084d",
    "deepnote_cell_height": 350.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 17532,
    "execution_start": 1653398898518,
    "source_hash": "8d28b685"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 14:54:07.766766: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-27 14:54:09.318382: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "#importing bert models\n",
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\")\n",
    "\n",
    "# Bert layers\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# Neural network layers\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "# Use inputs and outputs to construct a final model\n",
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_id": "00020-3a945c12-d539-4df6-ad07-71aaaea422ef",
    "deepnote_cell_height": 544.125,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     21.109375
    ],
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 27019508,
    "execution_start": 1653398916058,
    "source_hash": "86bb1f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "191/191 [==============================] - 207s 1s/step - loss: 0.6089 - accuracy: 0.6696 - val_loss: 0.5396 - val_accuracy: 0.7426\n",
      "Epoch 2/15\n",
      "191/191 [==============================] - 199s 1s/step - loss: 0.5487 - accuracy: 0.7332 - val_loss: 0.5135 - val_accuracy: 0.7636\n",
      "Epoch 3/15\n",
      "191/191 [==============================] - 203s 1s/step - loss: 0.5274 - accuracy: 0.7499 - val_loss: 0.5008 - val_accuracy: 0.7669\n",
      "Epoch 4/15\n",
      "191/191 [==============================] - 215s 1s/step - loss: 0.5215 - accuracy: 0.7535 - val_loss: 0.4941 - val_accuracy: 0.7682\n",
      "Epoch 5/15\n",
      "191/191 [==============================] - 225s 1s/step - loss: 0.5112 - accuracy: 0.7553 - val_loss: 0.4896 - val_accuracy: 0.7748\n",
      "Epoch 6/15\n",
      "191/191 [==============================] - 241s 1s/step - loss: 0.5041 - accuracy: 0.7645 - val_loss: 0.4961 - val_accuracy: 0.7610\n",
      "Epoch 7/15\n",
      "191/191 [==============================] - 210s 1s/step - loss: 0.5046 - accuracy: 0.7649 - val_loss: 0.4835 - val_accuracy: 0.7748\n",
      "Epoch 8/15\n",
      "191/191 [==============================] - 215s 1s/step - loss: 0.5008 - accuracy: 0.7701 - val_loss: 0.4875 - val_accuracy: 0.7676\n",
      "Epoch 9/15\n",
      "191/191 [==============================] - 209s 1s/step - loss: 0.4952 - accuracy: 0.7716 - val_loss: 0.4874 - val_accuracy: 0.7787\n",
      "Epoch 10/15\n",
      "191/191 [==============================] - 210s 1s/step - loss: 0.4964 - accuracy: 0.7696 - val_loss: 0.4792 - val_accuracy: 0.7807\n",
      "Epoch 11/15\n",
      "191/191 [==============================] - 211s 1s/step - loss: 0.4980 - accuracy: 0.7670 - val_loss: 0.4787 - val_accuracy: 0.7774\n",
      "Epoch 12/15\n",
      "191/191 [==============================] - 192s 1s/step - loss: 0.4957 - accuracy: 0.7706 - val_loss: 0.4814 - val_accuracy: 0.7774\n",
      "Epoch 13/15\n",
      "191/191 [==============================] - 211s 1s/step - loss: 0.4908 - accuracy: 0.7714 - val_loss: 0.4756 - val_accuracy: 0.7814\n",
      "Epoch 14/15\n",
      "191/191 [==============================] - 223s 1s/step - loss: 0.4916 - accuracy: 0.7713 - val_loss: 0.4751 - val_accuracy: 0.7761\n",
      "Epoch 15/15\n",
      "191/191 [==============================] - 207s 1s/step - loss: 0.4905 - accuracy: 0.7706 - val_loss: 0.4744 - val_accuracy: 0.7781\n"
     ]
    }
   ],
   "source": [
    "#compiling & training BERT model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,epochs=15, batch_size = 32, validation_data= (X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "00021-9ef8fe4c-4217-4b91-8b9b-e5c95e46d5fd",
    "deepnote_cell_height": 180.03125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 257,
    "execution_start": 1653461820490,
    "source_hash": "7dff0ec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50622815 0.12553826 0.7523049  ... 0.8410486  0.18414131 0.36373973]\n"
     ]
    }
   ],
   "source": [
    "#prediction of BERT\n",
    "y_predicted_bert = model.predict(X_test)\n",
    "y_predicted_bert = y_predicted_bert.flatten()\n",
    "print(y_predicted_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_id": "00022-75a0a21e-02b5-4d1e-a32c-f17ff0e31af5",
    "deepnote_cell_height": 80.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "9b08f9d7"
   },
   "outputs": [],
   "source": [
    "rounded_pre_bert = [round(num) for num in y_predicted_bert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_id": "00023-2e1bb89d-d8d9-49cc-b2cb-8fe56e97f4f6",
    "deepnote_cell_height": 222.078125,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "6ece2685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7704566349984676\n",
      "Precision:  0.754894283476899\n",
      "Recall:  0.6885714285714286\n",
      "F1-Measure:  0.7202091893911095\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_test, rounded_pre_bert))\n",
    "print(\"Precision: \", precision_score(y_test, rounded_pre_bert))\n",
    "print(\"Recall: \", recall_score(y_test, rounded_pre_bert))\n",
    "print(\"F1-Measure: \", f1_score(y_test, rounded_pre_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-ebda2ad9-1c57-4244-ab72-90b2932f79d8",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_id": "c9223aae9f0540ea845cbdc379703da9",
    "deepnote_cell_height": 530.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "c4fe78be",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  tf.Tensor(\n",
      "[b'The good thing #Royals face newbie playoffs. No real reason panic.'\n",
      " b'Skinny Jeans Hazardous Your Health! #socialnews http://t.co/LTMa9xQXpx'\n",
      " b'I added video @YouTube playlist http://t.co/v2yXurne2p Natural Disaster Survival - HUG BY A GUEST!! Roblox'\n",
      " ...\n",
      " b'Hundreds feared drowned another Mediterranean asylum seeker boat sinking http://t.co/zsYkzj2bzG'\n",
      " b'Trouble trouble I get way ????'\n",
      " b'Bayelsa poll: Tension Bayelsa Patience Jonathan plan hijack APC PDP: Plans former First Lady and... http://t.co/3eJL9lZlCH'], shape=(6090,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#creating the encoder for LSTM\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "\n",
    "print(\"Train: \", X_train)\n",
    "\n",
    "VOCAB_SIZE = 30000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(tf.convert_to_tensor(data[\"lemmatized_tweets_joined\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "1dc364ac02494105a5669d7d60aff1d3",
    "deepnote_cell_height": 260.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "52c5149",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating the simple LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_id": "5a3e3c7ee72a45689e8f1ccafd859542",
    "deepnote_cell_height": 134.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "d3956d1a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#compiling LSTM\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "cell_id": "ac6ae7d6e70b4fe5aba45d25a78dd0fb",
    "deepnote_cell_height": 80.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "914dacd8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "191/191 [==============================] - 18s 49ms/step - loss: 0.6591 - accuracy: 0.5966 - precision_30: 0.5200 - recall_23: 0.7354 - val_loss: 0.6759 - val_accuracy: 0.5837 - val_precision_30: 1.0000 - val_recall_23: 0.0231\n",
      "Epoch 2/15\n",
      "191/191 [==============================] - 6s 32ms/step - loss: 0.5139 - accuracy: 0.7634 - precision_30: 0.7198 - recall_23: 0.7312 - val_loss: 0.5940 - val_accuracy: 0.7459 - val_precision_30: 0.8994 - val_recall_23: 0.4545\n",
      "Epoch 3/15\n",
      "191/191 [==============================] - 6s 31ms/step - loss: 0.3567 - accuracy: 0.8489 - precision_30: 0.8559 - recall_23: 0.7776 - val_loss: 0.4794 - val_accuracy: 0.7787 - val_precision_30: 0.8628 - val_recall_23: 0.5716\n",
      "Epoch 4/15\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.2482 - accuracy: 0.9023 - precision_30: 0.9194 - recall_23: 0.8456 - val_loss: 0.4857 - val_accuracy: 0.7820 - val_precision_30: 0.7577 - val_recall_23: 0.7180\n",
      "Epoch 5/15\n",
      "191/191 [==============================] - 6s 31ms/step - loss: 0.1645 - accuracy: 0.9401 - precision_30: 0.9487 - recall_23: 0.9090 - val_loss: 0.5817 - val_accuracy: 0.7754 - val_precision_30: 0.7512 - val_recall_23: 0.7072\n",
      "Epoch 6/15\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.1185 - accuracy: 0.9560 - precision_30: 0.9570 - recall_23: 0.9393 - val_loss: 0.6767 - val_accuracy: 0.7649 - val_precision_30: 0.7374 - val_recall_23: 0.6965\n",
      "Epoch 7/15\n",
      "191/191 [==============================] - 6s 32ms/step - loss: 0.0878 - accuracy: 0.9680 - precision_30: 0.9678 - recall_23: 0.9570 - val_loss: 0.7633 - val_accuracy: 0.7492 - val_precision_30: 0.6955 - val_recall_23: 0.7319\n",
      "Epoch 8/15\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0749 - accuracy: 0.9731 - precision_30: 0.9736 - recall_23: 0.9631 - val_loss: 0.8068 - val_accuracy: 0.7492 - val_precision_30: 0.7171 - val_recall_23: 0.6795\n",
      "Epoch 9/15\n",
      "191/191 [==============================] - 6s 32ms/step - loss: 0.0649 - accuracy: 0.9760 - precision_30: 0.9742 - recall_23: 0.9697 - val_loss: 0.8369 - val_accuracy: 0.7413 - val_precision_30: 0.6983 - val_recall_23: 0.6918\n",
      "Epoch 10/15\n",
      "191/191 [==============================] - 6s 32ms/step - loss: 0.0513 - accuracy: 0.9806 - precision_30: 0.9799 - recall_23: 0.9747 - val_loss: 0.9434 - val_accuracy: 0.7380 - val_precision_30: 0.6771 - val_recall_23: 0.7365\n",
      "Epoch 11/15\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0501 - accuracy: 0.9837 - precision_30: 0.9827 - recall_23: 0.9793 - val_loss: 0.9193 - val_accuracy: 0.7479 - val_precision_30: 0.7093 - val_recall_23: 0.6918\n",
      "Epoch 12/15\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0370 - accuracy: 0.9870 - precision_30: 0.9876 - recall_23: 0.9820 - val_loss: 0.9715 - val_accuracy: 0.7538 - val_precision_30: 0.7246 - val_recall_23: 0.6810\n",
      "Epoch 13/15\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0471 - accuracy: 0.9859 - precision_30: 0.9827 - recall_23: 0.9843 - val_loss: 0.9544 - val_accuracy: 0.7446 - val_precision_30: 0.7031 - val_recall_23: 0.6934\n",
      "Epoch 14/15\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0297 - accuracy: 0.9913 - precision_30: 0.9923 - recall_23: 0.9873 - val_loss: 1.0193 - val_accuracy: 0.7544 - val_precision_30: 0.7311 - val_recall_23: 0.6703\n",
      "Epoch 15/15\n",
      "191/191 [==============================] - 6s 32ms/step - loss: 0.0288 - accuracy: 0.9901 - precision_30: 0.9911 - recall_23: 0.9858 - val_loss: 1.0611 - val_accuracy: 0.7420 - val_precision_30: 0.6945 - val_recall_23: 0.7042\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "history = model.fit(X_train, y_train, epochs = 15, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_lstm = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "cell_id": "00032-e296dc2d-82ee-43de-ab22-3f474d756d22",
    "deepnote_cell_height": 134.6875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "340ae96b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7848473703567488\n",
      "Precision:  0.7937701396348013\n",
      "Recall:  0.6528268551236749\n",
      "F1-Measure:  0.7164323800290839\n"
     ]
    }
   ],
   "source": [
    "#final metrics from the LSTM model.\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_predicted_lstm))\n",
    "print(\"Precision: \", precision_score(y_test, y_predicted_lstm))\n",
    "print(\"Recall: \", recall_score(y_test, y_predicted_lstm))\n",
    "print(\"F1-Measure: \", f1_score(y_test, y_predicted_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6cc5b975-c378-4446-8b15-03c3047279ca' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "0f965c63-46de-49b5-8d66-16e9b7abfc2f",
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
